{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb7a084",
   "metadata": {
    "id": "a5a8f6ac"
   },
   "source": [
    "# Scrapping Data Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d64c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a45e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the ChromeDriver\n",
    "chrome_driver_path = 'chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Path to the Chrome binary\n",
    "chrome_binary_path = 'chrome-win64/chrome.exe'\n",
    "\n",
    "# Setting up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = chrome_binary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a4a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the ChromeDriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da326d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"chemical weapon\"  # Replace with your search keyword\n",
    "start_date = \"2021-10-19\"  # Start date for search\n",
    "end_date = \"2024-10-18\"  # End date for search\n",
    "url = f\"https://twitter.com/search?q={query}%20since%3A{start_date}%20until%3A{end_date}&src=typed_query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba12d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f07991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816af95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4936b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ef21889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Path to the ChromeDriver and Chrome binary\n",
    "chrome_driver_path = 'chromedriver-win64/chromedriver.exe'  # Update with your actual path\n",
    "chrome_binary_path = 'chrome-win64/chrome.exe'  # Update with your actual path\n",
    "\n",
    "# Setting up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = chrome_binary_path\n",
    "# chrome_options.add_argument(\"--headless\")  # Run in headless mode (no browser window)\n",
    "# chrome_options.add_argument(\"--disable-gpu\")\n",
    "# chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initializing the ChromeDriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb501280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets for keyword: battles\n",
      "Saved 10 tweets for keyword 'battles' to tweets_battles.csv\n",
      "Pausing before the next keyword to avoid rate-limiting...\n"
     ]
    }
   ],
   "source": [
    "# Keywords to search for\n",
    "keywords = [\n",
    "    \"battles\"\n",
    "]\n",
    "\n",
    "# Date range for tweets\n",
    "start_date = \"2021-10-19\"\n",
    "end_date = \"2024-10-18\"\n",
    "\n",
    "# Number of tweets to retrieve per keyword\n",
    "tweet_limit_per_keyword = 10\n",
    "\n",
    "\n",
    "# Function to scrape tweets for a given keyword\n",
    "def scrape_tweets(keyword):\n",
    "    # Build the Twitter search URL\n",
    "    url = f\"https://twitter.com/search?q={keyword}%20since%3A{start_date}%20until%3A{end_date}&src=typed_query\"\n",
    "\n",
    "    # Initialize the WebDriver and open the URL\n",
    "#     driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    tweets = []\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Initial wait to load the page\n",
    "\n",
    "        # Initialize counters and status\n",
    "        tweets_collected = 0  # Counter for tweets collected\n",
    "        scroll_attempts = 0   # Number of scroll attempts\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")  # Get the initial scroll height\n",
    "        \n",
    "        while tweets_collected < tweet_limit_per_keyword:\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find each tweet container\n",
    "            tweet_divs = soup.find_all('div', {'data-testid': 'tweetText'})\n",
    "            for tweet_div in tweet_divs:\n",
    "                try:\n",
    "                    text = tweet_div.get_text()\n",
    "                    \n",
    "                    if text not in tweets:  # Avoid duplicates\n",
    "                        tweets.append(text)\n",
    "                        tweets_collected += 1\n",
    "#                         print(f\"Collected tweet {tweets_collected}: {text[:50]}...\")  # Display a preview of each tweet\n",
    "\n",
    "                    # Stop if we've reached the tweet limit for this keyword\n",
    "                    if tweets_collected >= tweet_limit_per_keyword:\n",
    "                        break\n",
    "\n",
    "                except Exception as e:\n",
    "                    time.sleep(2)\n",
    "#                     print(f\"An error occurred with a tweet: {e}\")\n",
    "                    continue  # Skip to the next tweet if an error occurs\n",
    "\n",
    "            \n",
    "            \n",
    "            # Stop if we have collected enough tweets\n",
    "            if tweets_collected >= tweet_limit_per_keyword:\n",
    "                break\n",
    "            \n",
    "            # Scroll down to load more tweets\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)  # Wait to allow tweets to load\n",
    "\n",
    "            # Check if the page height has changed to detect the end of the feed\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                time.sleep(2)\n",
    "#                 print(\"No more new tweets loaded, stopping.\")\n",
    "                break  # Exit if we can't scroll further\n",
    "\n",
    "            last_height = new_height  # Update the scroll height\n",
    "            scroll_attempts += 1\n",
    "#             print(f\"Scrolling attempt {scroll_attempts}, tweets collected: {tweets_collected}\")\n",
    "\n",
    "\n",
    "            time.sleep(3)  # Wait to allow tweets to load\n",
    "\n",
    "            # Additional short pause for anti-bot measures\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(2)\n",
    "#         print(f\"An error occurred while scraping {keyword}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        time.sleep(2)\n",
    "#         driver.quit()  # Ensure the driver is closed properly\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Loop through each keyword and save tweets to a separate CSV file\n",
    "for keyword in keywords:\n",
    "    print(f\"Scraping tweets for keyword: {keyword}\")\n",
    "    tweets = scrape_tweets(keyword)\n",
    "    \n",
    "    # Save tweets to a CSV file for each keyword\n",
    "    df = pd.DataFrame(tweets, columns=[\"Tweet\"])\n",
    "    filename = f\"tweets_{keyword.replace(' ', '_')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(tweets)} tweets for keyword '{keyword}' to {filename}\")\n",
    "\n",
    "    # Pause between keyword searches to avoid triggering anti-bot mechanisms\n",
    "    print(f\"Pausing before the next keyword to avoid rate-limiting...\")\n",
    "    time.sleep(10)  # Longer pause between keyword searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c926c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1589c48",
   "metadata": {},
   "source": [
    "# FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets for keyword: battles\n",
      "Scraping tweets for keyword: explosions\n",
      "Scraping tweets for keyword: remote violence\n",
      "Scraping tweets for keyword: protests\n",
      "Scraping tweets for keyword: riots\n",
      "Scraping tweets for keyword: strategic developments\n",
      "Scraping tweets for keyword: violence against civilians\n",
      "Scraping tweets for keyword: looting\n",
      "Scraping tweets for keyword: property destruction\n",
      "Scraping tweets for keyword: peaceful protest\n",
      "Scraping tweets for keyword: air strike\n"
     ]
    }
   ],
   "source": [
    "# Keywords to search for\n",
    "keywords = [\n",
    "    \"battles\", \"explosions\", \"remote violence\", \"protests\", \"riots\",\n",
    "    \"strategic developments\", \"violence against civilians\",\n",
    "    \"looting\", \"property destruction\", \"peaceful protest\", \"air strike\",\n",
    "    \"attack\", \"armed clash\", \"mob violence\", \"abduction\", \"disappearance\",\n",
    "    \"IED\", \"sexual violence\", \"chemical weapon\"\n",
    "]\n",
    "\n",
    "# Date range for tweets\n",
    "start_date = \"2021-10-19\"\n",
    "end_date = \"2024-10-18\"\n",
    "\n",
    "# Number of tweets to retrieve per keyword\n",
    "tweet_limit_per_keyword = 1000\n",
    "\n",
    "\n",
    "# Function to scrape tweets for a given keyword\n",
    "def scrape_tweets(keyword):\n",
    "    # Build the Twitter search URL\n",
    "    url = f\"https://twitter.com/search?q={keyword}%20since%3A{start_date}%20until%3A{end_date}&src=typed_query\"\n",
    "\n",
    "    # Initialize the WebDriver and open the URL\n",
    "#     driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    tweets = []\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Initial wait to load the page\n",
    "\n",
    "        # Initialize counters and status\n",
    "        tweets_collected = 0  # Counter for tweets collected\n",
    "        scroll_attempts = 0   # Number of scroll attempts\n",
    "        max_scroll_attempts = 2  # Maximum scroll attempts if no new tweets load\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")  # Get the initial scroll height\n",
    "        \n",
    "        while tweets_collected < tweet_limit_per_keyword:\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find each tweet container\n",
    "            tweet_divs = soup.find_all('article', {'data-testid': 'tweet'})\n",
    "            for tweet_div in tweet_divs:\n",
    "                try:\n",
    "                    # Extract tweet text\n",
    "                    text_div = tweet_div.find('div', {'data-testid': 'tweetText'})\n",
    "                    text = text_div.get_text() if text_div else \"\"\n",
    "\n",
    "                    # Extract tweet date\n",
    "                    date_div = tweet_div.find('div', {'data-testid': 'User-Name'})\n",
    "                    time_tag = date_div.find('time') if date_div else None\n",
    "                    tweet_date = time_tag['datetime'][:10] if time_tag else \"\"  # Format date as \"YYYY-MM-DD\"\n",
    "\n",
    "                    if text and tweet_date and {'text': text, 'date': tweet_date} not in tweets:  # Avoid duplicates\n",
    "                        tweets.append({'text': text, 'date': tweet_date})\n",
    "                        tweets_collected += 1\n",
    "#                         print(f\"Collected tweet {tweets_collected}: {text[:50]}... Date: {tweet_date}\")  # Display tweet and date preview\n",
    "\n",
    "                    # Stop if we've reached the tweet limit for this keyword\n",
    "                    if tweets_collected >= tweet_limit_per_keyword:\n",
    "                        break\n",
    "                \n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    time.sleep(2)\n",
    "#                     print(f\"An error occurred with a tweet: {e}\")\n",
    "                    continue  # Skip to the next tweet if an error occurs\n",
    "\n",
    "            \n",
    "            \n",
    "            # Stop if we have collected enough tweets\n",
    "            if tweets_collected >= tweet_limit_per_keyword:\n",
    "                break\n",
    "            \n",
    "            # Scroll down to load more tweets\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)  # Wait to allow tweets to load\n",
    "\n",
    "            # Check if the page height has changed to detect the end of the feed\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                scroll_attempts += 1\n",
    "#                 print(f\"No new tweets loaded. Scroll attempt {scroll_attempts}/{max_scroll_attempts}\")\n",
    "                \n",
    "                # Stop if maximum scroll attempts are reached\n",
    "                if scroll_attempts >= max_scroll_attempts:\n",
    "#                     print(\"Maximum scroll attempts reached. Moving to the next keyword.\")\n",
    "                    break\n",
    "            else:\n",
    "                # Reset scroll attempts if new tweets are loaded\n",
    "                scroll_attempts = 0\n",
    "\n",
    "            last_height = new_height  # Update the scroll height\n",
    "            scroll_attempts += 1\n",
    "#             print(f\"Scrolling attempt {scroll_attempts}, tweets collected: {tweets_collected}\")\n",
    "\n",
    "\n",
    "            time.sleep(3)  # Wait to allow tweets to load\n",
    "\n",
    "            # Additional short pause for anti-bot measures\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(2)\n",
    "#         print(f\"An error occurred while scraping {keyword}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        time.sleep(2)\n",
    "#         driver.quit()  # Ensure the driver is closed properly\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Loop through each keyword and save tweets to a separate CSV file\n",
    "for keyword in keywords:\n",
    "    print(f\"Scraping tweets for keyword: {keyword}\")\n",
    "    tweets = scrape_tweets(keyword)\n",
    "    \n",
    "    # Convert tweets to a DataFrame with 'Tweet' and 'Date' columns\n",
    "    df = pd.DataFrame(tweets, columns=[\"text\", \"date\"])\n",
    "    \n",
    "    # Rename columns to \"Tweet\" and \"Date\" for clarity in the CSV file\n",
    "    df = df.rename(columns={\"text\": \"Tweet\", \"date\": \"Date\"})\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    filename = f\"tweets_{keyword.replace(' ', '_')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "#     print(f\"Saved {len(tweets)} tweets for keyword '{keyword}' to {filename}\")\n",
    "\n",
    "    # Pause between keyword searches to avoid rate-limiting\n",
    "#     print(\"Pausing before the next keyword to avoid rate-limiting...\")\n",
    "    time.sleep(10)  # Longer pause between keyword searches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eff3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets for keyword: battle\n",
      "Scraping tweets for keyword: combat\n",
      "Scraping tweets for keyword: skirmish\n",
      "Scraping tweets for keyword: explosion\n",
      "Scraping tweets for keyword: blast\n",
      "Scraping tweets for keyword: detonation\n",
      "Scraping tweets for keyword: remote attack\n",
      "Scraping tweets for keyword: distance violence\n",
      "Scraping tweets for keyword: proxy violence\n",
      "Scraping tweets for keyword: protest\n",
      "Scraping tweets for keyword: demonstration\n",
      "Scraping tweets for keyword: rally\n",
      "Scraping tweets for keyword: riot\n",
      "Scraping tweets for keyword: civil unrest\n",
      "Scraping tweets for keyword: clashes\n",
      "Scraping tweets for keyword: strategic change\n",
      "Scraping tweets for keyword: policy shift\n",
      "Scraping tweets for keyword: geopolitical event\n",
      "Scraping tweets for keyword: civilian violence\n",
      "Scraping tweets for keyword: attacks on civilians\n"
     ]
    }
   ],
   "source": [
    "# Keywords to search for\n",
    "keywords = ['battle',\n",
    " 'combat',\n",
    " 'skirmish',\n",
    " 'explosion',\n",
    " 'blast',\n",
    " 'detonation',\n",
    " 'remote attack',\n",
    " 'distance violence',\n",
    " 'proxy violence',\n",
    " 'protest',\n",
    " 'demonstration',\n",
    " 'rally',\n",
    " 'riot',\n",
    " 'civil unrest',\n",
    " 'clashes',\n",
    " 'strategic change',\n",
    " 'policy shift',\n",
    " 'geopolitical event',\n",
    " 'civilian violence',\n",
    " 'attacks on civilians',\n",
    " 'civilian casualties',\n",
    " 'theft',\n",
    " 'plundering',\n",
    " 'raiding',\n",
    " 'vandalism',\n",
    " 'damage to property',\n",
    " 'infrastructure damage',\n",
    " 'nonviolent protest',\n",
    " 'peaceful demonstration',\n",
    " 'peace rally',\n",
    " 'airstrike',\n",
    " 'air raid',\n",
    " 'aerial attack',\n",
    " 'assault',\n",
    " 'strike',\n",
    " 'offensive',\n",
    " 'armed confrontation',\n",
    " 'firefight',\n",
    " 'armed encounter',\n",
    " 'group violence',\n",
    " 'crowd attack',\n",
    " 'mass assault',\n",
    " 'kidnapping',\n",
    " 'capture',\n",
    " 'hostage-taking',\n",
    " 'missing person',\n",
    " 'vanishing',\n",
    " 'abduction case',\n",
    " 'improvised explosive device',\n",
    " 'bomb',\n",
    " 'explosive',\n",
    " 'sexual assault',\n",
    " 'harassment',\n",
    " 'abuse',\n",
    " 'chemical attack',\n",
    " 'toxic weapon',\n",
    " 'poison gas',\n",
    " 'military operation',\n",
    " 'civil war',\n",
    " 'genocide',\n",
    " 'ethnic violence',\n",
    " 'human rights violation',\n",
    " 'martial law',\n",
    " 'terrorist attack',\n",
    " 'guerrilla warfare',\n",
    " 'cyber attack',\n",
    " 'human trafficking',\n",
    " 'forced migration',\n",
    " 'refugee crisis',\n",
    " 'drone strike',\n",
    " 'border conflict',\n",
    " 'cross-border attack',\n",
    " 'state violence',\n",
    " 'mass shooting',\n",
    " 'political unrest',\n",
    " 'rebellion',\n",
    " 'counter-insurgency']\n",
    "\n",
    "\n",
    "# Date range for tweets\n",
    "start_date = \"2021-10-19\"\n",
    "end_date = \"2024-10-18\"\n",
    "\n",
    "# Number of tweets to retrieve per keyword\n",
    "tweet_limit_per_keyword = 1000\n",
    "\n",
    "\n",
    "# Function to scrape tweets for a given keyword\n",
    "def scrape_tweets(keyword):\n",
    "    # Build the Twitter search URL\n",
    "    url = f\"https://twitter.com/search?q={keyword}%20since%3A{start_date}%20until%3A{end_date}&src=typed_query\"\n",
    "\n",
    "    # Initialize the WebDriver and open the URL\n",
    "#     driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    tweets = []\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Initial wait to load the page\n",
    "\n",
    "        # Initialize counters and status\n",
    "        tweets_collected = 0  # Counter for tweets collected\n",
    "        scroll_attempts = 0   # Number of scroll attempts\n",
    "        max_scroll_attempts = 2  # Maximum scroll attempts if no new tweets load\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")  # Get the initial scroll height\n",
    "        \n",
    "        while tweets_collected < tweet_limit_per_keyword:\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find each tweet container\n",
    "            tweet_divs = soup.find_all('article', {'data-testid': 'tweet'})\n",
    "            for tweet_div in tweet_divs:\n",
    "                try:\n",
    "                    # Extract tweet text\n",
    "                    text_div = tweet_div.find('div', {'data-testid': 'tweetText'})\n",
    "                    text = text_div.get_text() if text_div else \"\"\n",
    "\n",
    "                    # Extract tweet date\n",
    "                    date_div = tweet_div.find('div', {'data-testid': 'User-Name'})\n",
    "                    time_tag = date_div.find('time') if date_div else None\n",
    "                    tweet_date = time_tag['datetime'][:10] if time_tag else \"\"  # Format date as \"YYYY-MM-DD\"\n",
    "\n",
    "                    if text and tweet_date and {'text': text, 'date': tweet_date} not in tweets:  # Avoid duplicates\n",
    "                        tweets.append({'text': text, 'date': tweet_date})\n",
    "                        tweets_collected += 1\n",
    "#                         print(f\"Collected tweet {tweets_collected}: {text[:50]}... Date: {tweet_date}\")  # Display tweet and date preview\n",
    "\n",
    "                    # Stop if we've reached the tweet limit for this keyword\n",
    "                    if tweets_collected >= tweet_limit_per_keyword:\n",
    "                        break\n",
    "                \n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    time.sleep(2)\n",
    "#                     print(f\"An error occurred with a tweet: {e}\")\n",
    "                    continue  # Skip to the next tweet if an error occurs\n",
    "\n",
    "            \n",
    "            \n",
    "            # Stop if we have collected enough tweets\n",
    "            if tweets_collected >= tweet_limit_per_keyword:\n",
    "                break\n",
    "            \n",
    "            # Scroll down to load more tweets\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)  # Wait to allow tweets to load\n",
    "\n",
    "            # Check if the page height has changed to detect the end of the feed\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                scroll_attempts += 1\n",
    "#                 print(f\"No new tweets loaded. Scroll attempt {scroll_attempts}/{max_scroll_attempts}\")\n",
    "                \n",
    "                # Stop if maximum scroll attempts are reached\n",
    "                if scroll_attempts >= max_scroll_attempts:\n",
    "#                     print(\"Maximum scroll attempts reached. Moving to the next keyword.\")\n",
    "                    break\n",
    "            else:\n",
    "                # Reset scroll attempts if new tweets are loaded\n",
    "                scroll_attempts = 0\n",
    "\n",
    "            last_height = new_height  # Update the scroll height\n",
    "            scroll_attempts += 1\n",
    "#             print(f\"Scrolling attempt {scroll_attempts}, tweets collected: {tweets_collected}\")\n",
    "\n",
    "\n",
    "            time.sleep(3)  # Wait to allow tweets to load\n",
    "\n",
    "            # Additional short pause for anti-bot measures\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(2)\n",
    "#         print(f\"An error occurred while scraping {keyword}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        time.sleep(2)\n",
    "#         driver.quit()  # Ensure the driver is closed properly\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Loop through each keyword and save tweets to a separate CSV file\n",
    "for keyword in keywords:\n",
    "    print(f\"Scraping tweets for keyword: {keyword}\")\n",
    "    tweets = scrape_tweets(keyword)\n",
    "    \n",
    "    # Convert tweets to a DataFrame with 'Tweet' and 'Date' columns\n",
    "    df = pd.DataFrame(tweets, columns=[\"text\", \"date\"])\n",
    "    \n",
    "    # Rename columns to \"Tweet\" and \"Date\" for clarity in the CSV file\n",
    "    df = df.rename(columns={\"text\": \"Tweet\", \"date\": \"Date\"})\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    filename = f\"tweets_{keyword.replace(' ', '_')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "#     print(f\"Saved {len(tweets)} tweets for keyword '{keyword}' to {filename}\")\n",
    "\n",
    "    # Pause between keyword searches to avoid rate-limiting\n",
    "#     print(\"Pausing before the next keyword to avoid rate-limiting...\")\n",
    "    time.sleep(10)  # Longer pause between keyword searches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c205c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9e472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36101bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Twint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
